/**

@mainpage 15-410 Project 3

@author Name1 (id1)
@author Name2 (id2)

Same drill as last time.

TODO LIST:
implement rollbacks on resource failure
think about readline
think about ZFOD and COW
make all scheduler assigns atomic (xchng)
Make idle thread run runnable_pool is empty
Get rid of restore_context and instead return esp in exec and yield
Page directories should share same kernel page tables
check for valid pointers in exec
Implement thread_fork
Implement swexn
Look through all TODO comments

Design Decision Rationales:

New pages & remove pages - One interesting thing about remove pages is that
it does not need to know how long the length of the allocated chunk of memory
that data should be remembered from a call to new_pages. We do this by using 2
of the unused bits in the page table entry flags. Bit 9 denotes the beginning
of a new_pages allocation and bit 10 denotes the end of a new_pages allocation.
Therefore, if we call remove_pages on a memory location that does not have the
9th bit set, we can immediately return an error code. Otherwise, we simply
while loop and increase the virtual address until we find a mapping that has
the 10th bit set.

Sleeping pool design - In order to maintain constant time context switching,
we decided on implementing the sleeping pool as a linked list that maintains
an ordering. This allows us to check whether or not we need to wake up a thread
in constant time by simply looking at the first element - which has the lowest
lookup time - and comparing it to the current time for every tick. Granted, a
sorted insertion into a linked list takes O(n) time, this is very much worth
it for the constant time wakeup procedure.

Global scheduler lock - The reason we have a global scheduler lock
rather than one inside the scheduler is to avoid the following situation:
Consider thread 1 in fork() that grabs the scheduler's mutex. After it
grabs the mutex, a timer fires and a context switch is being serviced.
The context switcher also accesses the scheduler and tries to acquire
it's mutex, but it can't because thread 1 controls it. And thus,
execution stops because the scheduler waits forever.
With a global scheduler lock, syscalls the modify the scheduler are thread
safe.

Scheduler TCB pool data structure - Right from the beginning we abstracted
the tcb pool data structure out. This allowed us to change the internals
of the pool as the project progressed. For ck1 and ck2, we used a linked
list of tcb pools. Every pool access was O(n) where n = total number
of threads. Then, the runnable and waiting pools were queues and
not abstracted out. However, we encountered problems because our queue
implementation used malloc for every deq and enq; it was grossly
inefficient combined with the linked list of threads. We decided to
revamp the whole data structure to ensure least amount of mallocs and
constant access time.
We wrote a hashtable data structure capable of storing generic data with
an accompanying key. Then in the tcb_pool, we store linked list nodes
each holding a tcb into the hash table. The key is the tid of the stored
tcb. Essentially, it is a hash table of linked list nodes, where each
node belongs to either the runnable, waiting, or sleeping pools. This
allows each transfer of nodes between pools to be constant time, and
thus syscalls like deschedule and make_runnable O(1). Each tcb also has a
status that indicates which pool it belongs to. Although writing this
data structure was arduous, it proved to be a really good solution to
minimize access time and number of malloc/free pairs.

*/
